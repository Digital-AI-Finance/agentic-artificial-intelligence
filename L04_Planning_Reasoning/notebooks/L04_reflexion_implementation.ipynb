{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L04: Reflexion Implementation\n",
    "\n",
    "**Week 4 - Planning and Reasoning**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Reflexion framework for self-improvement\n",
    "- Implement verbal reflection and memory mechanisms\n",
    "- Build a simple Reflexion agent for problem-solving\n",
    "- Evaluate the impact of reflection on task performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Reflexion (Shinn et al., 2023) enables agents to learn from mistakes through verbal self-reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Callable\n",
    "import json\n",
    "\n",
    "print(\"Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Reflexion Components\n",
    "\n",
    "The Reflexion loop consists of:\n",
    "1. **Actor**: Attempts the task\n",
    "2. **Evaluator**: Assesses the outcome\n",
    "3. **Reflector**: Generates verbal feedback\n",
    "4. **Memory**: Stores reflections for future attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Attempt:\n",
    "    \"\"\"Record of a single task attempt.\"\"\"\n",
    "    action: str\n",
    "    result: str\n",
    "    success: bool\n",
    "    reflection: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class EpisodicMemory:\n",
    "    \"\"\"Stores past attempts and reflections.\"\"\"\n",
    "    attempts: List[Attempt] = field(default_factory=list)\n",
    "    max_reflections: int = 5\n",
    "    \n",
    "    def add_attempt(self, attempt: Attempt):\n",
    "        self.attempts.append(attempt)\n",
    "        # Keep only recent reflections\n",
    "        if len(self.attempts) > self.max_reflections:\n",
    "            self.attempts = self.attempts[-self.max_reflections:]\n",
    "    \n",
    "    def get_reflections(self) -> List[str]:\n",
    "        \"\"\"Return all stored reflections.\"\"\"\n",
    "        return [a.reflection for a in self.attempts if a.reflection]\n",
    "    \n",
    "    def get_context(self) -> str:\n",
    "        \"\"\"Format reflections as context for the next attempt.\"\"\"\n",
    "        reflections = self.get_reflections()\n",
    "        if not reflections:\n",
    "            return \"No previous attempts.\"\n",
    "        return \"Previous insights:\\n\" + \"\\n\".join(f\"- {r}\" for r in reflections)\n",
    "\n",
    "print(\"Memory classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflexionAgent:\n",
    "    \"\"\"Agent that learns from self-reflection.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 actor_fn: Callable,\n",
    "                 evaluator_fn: Callable,\n",
    "                 reflector_fn: Callable,\n",
    "                 max_attempts: int = 3):\n",
    "        self.actor = actor_fn\n",
    "        self.evaluator = evaluator_fn\n",
    "        self.reflector = reflector_fn\n",
    "        self.max_attempts = max_attempts\n",
    "        self.memory = EpisodicMemory()\n",
    "    \n",
    "    def solve(self, task: str) -> dict:\n",
    "        \"\"\"Attempt task with reflection loop.\"\"\"\n",
    "        for attempt_num in range(self.max_attempts):\n",
    "            print(f\"\\n--- Attempt {attempt_num + 1} ---\")\n",
    "            \n",
    "            # Get context from previous reflections\n",
    "            context = self.memory.get_context()\n",
    "            print(f\"Context: {context}\")\n",
    "            \n",
    "            # Actor generates solution\n",
    "            action = self.actor(task, context)\n",
    "            print(f\"Action: {action}\")\n",
    "            \n",
    "            # Evaluator checks result\n",
    "            result, success = self.evaluator(task, action)\n",
    "            print(f\"Result: {result}, Success: {success}\")\n",
    "            \n",
    "            if success:\n",
    "                return {\"success\": True, \"action\": action, \"attempts\": attempt_num + 1}\n",
    "            \n",
    "            # Reflector generates insight\n",
    "            reflection = self.reflector(task, action, result)\n",
    "            print(f\"Reflection: {reflection}\")\n",
    "            \n",
    "            # Store in memory\n",
    "            self.memory.add_attempt(Attempt(\n",
    "                action=action,\n",
    "                result=result,\n",
    "                success=success,\n",
    "                reflection=reflection\n",
    "            ))\n",
    "        \n",
    "        return {\"success\": False, \"attempts\": self.max_attempts}\n",
    "\n",
    "print(\"ReflexionAgent defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Math Problem Solving\n",
    "\n",
    "We'll simulate an agent learning to solve math problems through reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated LLM responses for math problem solving\n",
    "def math_actor(task: str, context: str) -> str:\n",
    "    \"\"\"Simulated actor that improves based on reflections.\"\"\"\n",
    "    # Check if we have learned from reflections\n",
    "    if \"check units\" in context.lower():\n",
    "        return \"The answer is 150 miles (calculated: 50 mph * 3 hours = 150 miles)\"\n",
    "    elif \"show work\" in context.lower():\n",
    "        return \"Speed = 50 mph, Time = 3 hours. Answer: 150\"\n",
    "    else:\n",
    "        # Initial attempt without learning\n",
    "        return \"The answer is 53\"  # Wrong answer\n",
    "\n",
    "def math_evaluator(task: str, action: str) -> tuple:\n",
    "    \"\"\"Check if the answer is correct.\"\"\"\n",
    "    correct_answer = \"150\"\n",
    "    if correct_answer in action and \"miles\" in action.lower():\n",
    "        return \"Correct!\", True\n",
    "    elif correct_answer in action:\n",
    "        return \"Missing units\", False\n",
    "    else:\n",
    "        return \"Incorrect calculation\", False\n",
    "\n",
    "def math_reflector(task: str, action: str, result: str) -> str:\n",
    "    \"\"\"Generate reflection based on failure.\"\"\"\n",
    "    if \"Incorrect calculation\" in result:\n",
    "        return \"I made an arithmetic error. I should show my work step by step.\"\n",
    "    elif \"Missing units\" in result:\n",
    "        return \"I forgot the units. I should always check units in my answer.\"\n",
    "    return \"I need to be more careful with my reasoning.\"\n",
    "\n",
    "print(\"Math problem functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Reflexion agent\n",
    "agent = ReflexionAgent(\n",
    "    actor_fn=math_actor,\n",
    "    evaluator_fn=math_evaluator,\n",
    "    reflector_fn=math_reflector,\n",
    "    max_attempts=3\n",
    ")\n",
    "\n",
    "task = \"A car travels at 50 mph for 3 hours. How far does it travel?\"\n",
    "print(f\"Task: {task}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = agent.solve(task)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Final Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reflection Quality Analysis\n",
    "\n",
    "Good reflections are:\n",
    "- Specific about what went wrong\n",
    "- Actionable for future attempts\n",
    "- Focused on the error pattern, not just the symptom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of good vs. bad reflections\n",
    "reflection_examples = {\n",
    "    \"Good Reflections\": [\n",
    "        \"I assumed the list was sorted but it wasn't. Always verify data assumptions.\",\n",
    "        \"Off-by-one error in loop. Use len(arr) - 1 for index-based iteration.\",\n",
    "        \"Forgot to handle empty input case. Add input validation first.\"\n",
    "    ],\n",
    "    \"Bad Reflections\": [\n",
    "        \"I was wrong.\",  # Too vague\n",
    "        \"Try harder next time.\",  # Not actionable\n",
    "        \"The test case was tricky.\"  # Blames external factors\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, examples in reflection_examples.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for ex in examples:\n",
    "        print(f\"  - {ex}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory Management Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PrioritizedMemory:\n",
    "    \"\"\"Memory that prioritizes recent and impactful reflections.\"\"\"\n",
    "    reflections: List[dict] = field(default_factory=list)\n",
    "    max_size: int = 10\n",
    "    \n",
    "    def add(self, reflection: str, importance: float = 1.0):\n",
    "        self.reflections.append({\n",
    "            \"text\": reflection,\n",
    "            \"importance\": importance,\n",
    "            \"uses\": 0\n",
    "        })\n",
    "        # Keep most important\n",
    "        self.reflections.sort(key=lambda x: x[\"importance\"], reverse=True)\n",
    "        self.reflections = self.reflections[:self.max_size]\n",
    "    \n",
    "    def retrieve(self, k: int = 3) -> List[str]:\n",
    "        \"\"\"Get top-k most relevant reflections.\"\"\"\n",
    "        top_k = sorted(self.reflections, \n",
    "                       key=lambda x: x[\"importance\"] - x[\"uses\"] * 0.1,\n",
    "                       reverse=True)[:k]\n",
    "        # Update use counts\n",
    "        for r in top_k:\n",
    "            r[\"uses\"] += 1\n",
    "        return [r[\"text\"] for r in top_k]\n",
    "\n",
    "# Test prioritized memory\n",
    "memory = PrioritizedMemory()\n",
    "memory.add(\"Check edge cases\", importance=0.9)\n",
    "memory.add(\"Verify loop bounds\", importance=0.8)\n",
    "memory.add(\"Handle null inputs\", importance=0.95)\n",
    "\n",
    "print(\"Top reflections:\", memory.retrieve(k=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n",
    "\n",
    "Reflexion effectiveness can be measured by:\n",
    "- **Pass@k**: Success rate within k attempts\n",
    "- **Reflection Quality**: Specificity and actionability\n",
    "- **Learning Curve**: Improvement rate across attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reflexion(agent, tasks: List[str], max_attempts: int = 3):\n",
    "    \"\"\"Evaluate Reflexion agent performance.\"\"\"\n",
    "    results = {\n",
    "        \"pass@1\": 0,\n",
    "        \"pass@2\": 0,\n",
    "        \"pass@3\": 0,\n",
    "        \"total\": len(tasks)\n",
    "    }\n",
    "    \n",
    "    for task in tasks:\n",
    "        # Reset agent memory for each task\n",
    "        agent.memory = EpisodicMemory()\n",
    "        result = agent.solve(task)\n",
    "        \n",
    "        if result[\"success\"]:\n",
    "            attempts = result[\"attempts\"]\n",
    "            for k in range(attempts, max_attempts + 1):\n",
    "                results[f\"pass@{k}\"] += 1\n",
    "    \n",
    "    # Convert to percentages\n",
    "    for k in range(1, max_attempts + 1):\n",
    "        results[f\"pass@{k}\"] = results[f\"pass@{k}\"] / results[\"total\"] * 100\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example metrics (simulated)\n",
    "print(\"Example Reflexion Results:\")\n",
    "print(\"  Pass@1: 80%\")\n",
    "print(\"  Pass@2: 88%\")\n",
    "print(\"  Pass@3: 91%\")\n",
    "print(\"\\nImprovement from reflection: +11%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "1. **Reflexion Loop**: Attempt -> Evaluate -> Reflect -> Store -> Retry\n",
    "2. **Verbal Feedback**: Natural language reflections are interpretable\n",
    "3. **Memory Management**: Prioritize relevant, actionable reflections\n",
    "4. **Quality Matters**: Specific, actionable reflections drive improvement\n",
    "\n",
    "## Next Steps\n",
    "- Integrate with real LLM APIs\n",
    "- Test on HumanEval coding benchmark\n",
    "- Experiment with different reflection prompts\n",
    "- Compare with other self-improvement methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
