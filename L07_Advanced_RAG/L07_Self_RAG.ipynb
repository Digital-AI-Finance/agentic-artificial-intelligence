{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Implementing Self-RAG\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Digital-AI-Finance/agentic-artificial-intelligence/blob/main/L07_Advanced_RAG/L07_Self_RAG.ipynb)\n",
    "\n",
    "This notebook implements Self-RAG concepts including:\n",
    "- Adaptive retrieval decisions\n",
    "- Relevance scoring and critique\n",
    "- Corrective retrieval actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab setup\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q langchain-openai langchain-community chromadb python-dotenv\n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "print(\"Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about AI agents\n",
    "documents = [\n",
    "    Document(page_content=\"ReAct is a paradigm that combines reasoning and acting in language models. It was proposed by Yao et al. in 2023.\", metadata={\"source\": \"react_paper\"}),\n",
    "    Document(page_content=\"Self-RAG trains LLMs to adaptively retrieve and critique information. It uses special tokens like [Retrieve] and [IsRel].\", metadata={\"source\": \"selfrag_paper\"}),\n",
    "    Document(page_content=\"CRAG evaluates retrieval quality with confidence scores and takes corrective actions when needed.\", metadata={\"source\": \"crag_paper\"}),\n",
    "    Document(page_content=\"LangGraph is a framework for building stateful agents using graph-based architectures.\", metadata={\"source\": \"langgraph_docs\"}),\n",
    "    Document(page_content=\"AutoGen enables multi-agent conversations with code execution capabilities.\", metadata={\"source\": \"autogen_docs\"}),\n",
    "    Document(page_content=\"The capital of France is Paris. It is known for the Eiffel Tower.\", metadata={\"source\": \"geography\"}),\n",
    "]\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(f\"Created vector store with {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RetrievalDecision:\n",
    "    \"\"\"Represents the decision to retrieve or not.\"\"\"\n",
    "    should_retrieve: bool\n",
    "    reasoning: str\n",
    "\n",
    "@dataclass\n",
    "class RelevanceScore:\n",
    "    \"\"\"Represents relevance evaluation of a document.\"\"\"\n",
    "    document: Document\n",
    "    is_relevant: bool\n",
    "    score: float\n",
    "    reasoning: str\n",
    "\n",
    "@dataclass\n",
    "class GenerationCritique:\n",
    "    \"\"\"Represents critique of generated answer.\"\"\"\n",
    "    is_supported: bool\n",
    "    is_useful: bool\n",
    "    confidence: float\n",
    "    reasoning: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_retrieval(query: str) -> RetrievalDecision:\n",
    "    \"\"\"Decide whether retrieval is needed for this query.\"\"\"\n",
    "    prompt = f\"\"\"Analyze this query and decide if external knowledge retrieval is needed.\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Consider:\n",
    "- Is this a factual question requiring specific knowledge?\n",
    "- Can this be answered from general knowledge?\n",
    "- Does this require recent or domain-specific information?\n",
    "\n",
    "Respond with:\n",
    "DECISION: [YES/NO]\n",
    "REASONING: [Your reasoning]\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt).content\n",
    "    should_retrieve = \"YES\" in response.upper().split(\"DECISION:\")[1].split(\"\\n\")[0]\n",
    "    reasoning = response.split(\"REASONING:\")[1].strip() if \"REASONING:\" in response else \"\"\n",
    "    \n",
    "    return RetrievalDecision(should_retrieve=should_retrieve, reasoning=reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(query: str, doc: Document) -> RelevanceScore:\n",
    "    \"\"\"Evaluate if a retrieved document is relevant to the query.\"\"\"\n",
    "    prompt = f\"\"\"Evaluate if this document is relevant to the query.\n",
    "\n",
    "Query: {query}\n",
    "Document: {doc.page_content}\n",
    "\n",
    "Rate relevance from 0.0 to 1.0 and explain.\n",
    "\n",
    "Respond with:\n",
    "SCORE: [0.0-1.0]\n",
    "RELEVANT: [YES/NO]\n",
    "REASONING: [Your reasoning]\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt).content\n",
    "    \n",
    "    try:\n",
    "        score = float(response.split(\"SCORE:\")[1].split(\"\\n\")[0].strip())\n",
    "    except:\n",
    "        score = 0.5\n",
    "    \n",
    "    is_relevant = \"YES\" in response.upper().split(\"RELEVANT:\")[1].split(\"\\n\")[0]\n",
    "    reasoning = response.split(\"REASONING:\")[1].strip() if \"REASONING:\" in response else \"\"\n",
    "    \n",
    "    return RelevanceScore(document=doc, is_relevant=is_relevant, score=score, reasoning=reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critique_generation(query: str, answer: str, sources: List[Document]) -> GenerationCritique:\n",
    "    \"\"\"Critique the generated answer.\"\"\"\n",
    "    source_text = \"\\n\".join([d.page_content for d in sources])\n",
    "    \n",
    "    prompt = f\"\"\"Critique this answer based on the sources.\n",
    "\n",
    "Query: {query}\n",
    "Answer: {answer}\n",
    "Sources: {source_text}\n",
    "\n",
    "Evaluate:\n",
    "1. Is the answer supported by the sources?\n",
    "2. Is the answer useful for the query?\n",
    "3. Overall confidence (0.0-1.0)?\n",
    "\n",
    "Respond with:\n",
    "SUPPORTED: [YES/NO]\n",
    "USEFUL: [YES/NO]\n",
    "CONFIDENCE: [0.0-1.0]\n",
    "REASONING: [Your reasoning]\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt).content\n",
    "    \n",
    "    is_supported = \"YES\" in response.split(\"SUPPORTED:\")[1].split(\"\\n\")[0].upper()\n",
    "    is_useful = \"YES\" in response.split(\"USEFUL:\")[1].split(\"\\n\")[0].upper()\n",
    "    \n",
    "    try:\n",
    "        confidence = float(response.split(\"CONFIDENCE:\")[1].split(\"\\n\")[0].strip())\n",
    "    except:\n",
    "        confidence = 0.5\n",
    "    \n",
    "    reasoning = response.split(\"REASONING:\")[1].strip() if \"REASONING:\" in response else \"\"\n",
    "    \n",
    "    return GenerationCritique(\n",
    "        is_supported=is_supported,\n",
    "        is_useful=is_useful,\n",
    "        confidence=confidence,\n",
    "        reasoning=reasoning\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query: str, max_retries: int = 2) -> Dict:\n",
    "    \"\"\"Complete Self-RAG pipeline.\"\"\"\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieval_decision\": None,\n",
    "        \"documents\": [],\n",
    "        \"relevance_scores\": [],\n",
    "        \"answer\": None,\n",
    "        \"critique\": None,\n",
    "        \"iterations\": 0\n",
    "    }\n",
    "    \n",
    "    # Step 1: Decide whether to retrieve\n",
    "    decision = decide_retrieval(query)\n",
    "    result[\"retrieval_decision\"] = decision\n",
    "    print(f\"[Retrieve?] {decision.should_retrieve} - {decision.reasoning[:100]}...\")\n",
    "    \n",
    "    if not decision.should_retrieve:\n",
    "        # Generate without retrieval\n",
    "        answer = llm.invoke(f\"Answer this question: {query}\").content\n",
    "        result[\"answer\"] = answer\n",
    "        return result\n",
    "    \n",
    "    # Step 2: Retrieve and evaluate\n",
    "    for iteration in range(max_retries):\n",
    "        result[\"iterations\"] = iteration + 1\n",
    "        \n",
    "        # Retrieve\n",
    "        docs = retriever.invoke(query)\n",
    "        result[\"documents\"] = docs\n",
    "        print(f\"[Retrieved] {len(docs)} documents\")\n",
    "        \n",
    "        # Evaluate relevance\n",
    "        relevance_scores = [evaluate_relevance(query, doc) for doc in docs]\n",
    "        result[\"relevance_scores\"] = relevance_scores\n",
    "        \n",
    "        relevant_docs = [rs.document for rs in relevance_scores if rs.is_relevant]\n",
    "        print(f\"[Relevant] {len(relevant_docs)}/{len(docs)} documents\")\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            print(f\"[Retry] No relevant documents, iteration {iteration + 1}\")\n",
    "            continue\n",
    "        \n",
    "        # Step 3: Generate with relevant documents\n",
    "        context = \"\\n\".join([d.page_content for d in relevant_docs])\n",
    "        answer = llm.invoke(\n",
    "            f\"Based on this context:\\n{context}\\n\\nAnswer: {query}\"\n",
    "        ).content\n",
    "        result[\"answer\"] = answer\n",
    "        \n",
    "        # Step 4: Critique generation\n",
    "        critique = critique_generation(query, answer, relevant_docs)\n",
    "        result[\"critique\"] = critique\n",
    "        print(f\"[Critique] Supported: {critique.is_supported}, Useful: {critique.is_useful}, Confidence: {critique.confidence}\")\n",
    "        \n",
    "        if critique.confidence >= 0.7:\n",
    "            break\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Query requiring retrieval\n",
    "result1 = self_rag(\"What is Self-RAG and how does it work?\")\n",
    "print(f\"\\nAnswer: {result1['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Query not requiring retrieval\n",
    "result2 = self_rag(\"What is 2 + 2?\")\n",
    "print(f\"\\nAnswer: {result2['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Domain-specific query\n",
    "result3 = self_rag(\"Compare ReAct and LangGraph approaches to building agents.\")\n",
    "print(f\"\\nAnswer: {result3['answer'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented Self-RAG concepts:\n",
    "1. **Retrieval Decision**: LLM decides whether to retrieve\n",
    "2. **Relevance Evaluation**: Score and filter retrieved documents\n",
    "3. **Generation Critique**: Evaluate answer quality\n",
    "4. **Iterative Refinement**: Retry on low confidence\n",
    "\n",
    "Key insights:\n",
    "- Adaptive retrieval reduces noise for simple queries\n",
    "- Relevance filtering improves context quality\n",
    "- Self-critique enables error detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
