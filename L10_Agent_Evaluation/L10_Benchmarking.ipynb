{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Agent Benchmarking\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Digital-AI-Finance/agentic-artificial-intelligence/blob/main/L10_Agent_Evaluation/L10_Benchmarking.ipynb)\n",
    "\n",
    "Building a simple agent evaluation framework with multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q langchain-openai python-dotenv\n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Callable\n",
    "from dataclasses import dataclass\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalResult:\n",
    "    task_id: str\n",
    "    success: bool\n",
    "    latency_ms: float\n",
    "    token_count: int\n",
    "    steps: int\n",
    "    answer: str\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    success_rate: float\n",
    "    avg_latency_ms: float\n",
    "    avg_tokens: float\n",
    "    avg_steps: float\n",
    "    results: List[EvalResult]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.steps = 0\n",
    "        self.tokens = 0\n",
    "    \n",
    "    def solve(self, task: str) -> str:\n",
    "        self.steps = 0\n",
    "        self.tokens = 0\n",
    "        \n",
    "        # Step 1: Understand task\n",
    "        self.steps += 1\n",
    "        understanding = self.llm.invoke(f\"Briefly explain what this task requires: {task}\").content\n",
    "        self.tokens += len(understanding.split())\n",
    "        \n",
    "        # Step 2: Plan\n",
    "        self.steps += 1\n",
    "        plan = self.llm.invoke(f\"Create a step-by-step plan to solve: {task}\").content\n",
    "        self.tokens += len(plan.split())\n",
    "        \n",
    "        # Step 3: Execute\n",
    "        self.steps += 1\n",
    "        answer = self.llm.invoke(f\"Execute this plan and provide the answer:\\nTask: {task}\\nPlan: {plan}\").content\n",
    "        self.tokens += len(answer.split())\n",
    "        \n",
    "        return answer\n",
    "\n",
    "agent = SimpleAgent(llm)\n",
    "print(\"Agent ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task(agent: SimpleAgent, task: Dict, judge_llm) -> EvalResult:\n",
    "    \"\"\"Evaluate agent on a single task.\"\"\"\n",
    "    start = time.time()\n",
    "    answer = agent.solve(task[\"question\"])\n",
    "    latency = (time.time() - start) * 1000\n",
    "    \n",
    "    # Judge correctness\n",
    "    judge_prompt = f\"\"\"Is this answer correct for the question?\n",
    "Question: {task['question']}\n",
    "Expected: {task['expected']}\n",
    "Actual: {answer}\n",
    "\n",
    "Reply YES or NO only.\"\"\"\n",
    "    \n",
    "    judgment = judge_llm.invoke(judge_prompt).content.strip().upper()\n",
    "    success = \"YES\" in judgment\n",
    "    \n",
    "    return EvalResult(\n",
    "        task_id=task[\"id\"],\n",
    "        success=success,\n",
    "        latency_ms=latency,\n",
    "        token_count=agent.tokens,\n",
    "        steps=agent.steps,\n",
    "        answer=answer\n",
    "    )\n",
    "\n",
    "def run_benchmark(agent: SimpleAgent, tasks: List[Dict]) -> BenchmarkResult:\n",
    "    \"\"\"Run full benchmark.\"\"\"\n",
    "    results = []\n",
    "    judge = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    for i, task in enumerate(tasks):\n",
    "        print(f\"Evaluating task {i+1}/{len(tasks)}...\")\n",
    "        result = evaluate_task(agent, task, judge)\n",
    "        results.append(result)\n",
    "        print(f\"  {'PASS' if result.success else 'FAIL'} ({result.latency_ms:.0f}ms)\")\n",
    "    \n",
    "    return BenchmarkResult(\n",
    "        success_rate=sum(r.success for r in results) / len(results),\n",
    "        avg_latency_ms=sum(r.latency_ms for r in results) / len(results),\n",
    "        avg_tokens=sum(r.token_count for r in results) / len(results),\n",
    "        avg_steps=sum(r.steps for r in results) / len(results),\n",
    "        results=results\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample benchmark tasks\n",
    "tasks = [\n",
    "    {\"id\": \"math1\", \"question\": \"What is 15 * 23?\", \"expected\": \"345\"},\n",
    "    {\"id\": \"math2\", \"question\": \"What is the square root of 144?\", \"expected\": \"12\"},\n",
    "    {\"id\": \"logic1\", \"question\": \"If all cats are animals and some animals are pets, can we conclude all cats are pets?\", \"expected\": \"No\"},\n",
    "    {\"id\": \"knowledge1\", \"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"id\": \"knowledge2\", \"question\": \"Who wrote Romeo and Juliet?\", \"expected\": \"Shakespeare\"},\n",
    "]\n",
    "\n",
    "benchmark_result = run_benchmark(agent, tasks)\n",
    "\n",
    "print(f\"\\n=== Benchmark Results ===\")\n",
    "print(f\"Success Rate: {benchmark_result.success_rate:.1%}\")\n",
    "print(f\"Avg Latency: {benchmark_result.avg_latency_ms:.0f}ms\")\n",
    "print(f\"Avg Tokens: {benchmark_result.avg_tokens:.0f}\")\n",
    "print(f\"Avg Steps: {benchmark_result.avg_steps:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Built a simple agent benchmarking framework with:\n",
    "- **Metrics**: Success rate, latency, token usage, step count\n",
    "- **LLM-as-Judge**: Automated correctness evaluation\n",
    "- **Extensible**: Easy to add new task types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
