---
layout: week
title: "Week 2: LLM Foundations for Agents"
week_number: 2
nav_order: 2
parent: Weeks
---

## Exercise

Compare prompting strategies on a set of reasoning problems:
1. Implement zero-shot, few-shot, and Chain-of-Thought prompting
2. Measure accuracy and token usage
3. Analyze when each strategy is most effective

## Discussion Questions

1. When should you use Self-Consistency over standard CoT?
2. How does Tree-of-Thoughts trade off exploration vs. exploitation?
3. What role does temperature play in reasoning quality?

## Additional Resources

- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub)
