---
layout: week
title: "Week 10: Agent Evaluation"
week_number: 10
nav_order: 10
parent: Weeks
---

## Exercise

Design an evaluation framework for a specific agent type:
1. Define success metrics
2. Create a benchmark task suite
3. Implement LLM-as-Judge evaluation
4. Compare multiple agent architectures

## Discussion Questions

1. How do you prevent benchmark overfitting?
2. When is LLM-as-Judge reliable?
3. What makes a good human baseline?

## Additional Resources

- [AgentBench](https://github.com/THUDM/AgentBench)
- [WebArena](https://webarena.dev/)
- [SWE-bench](https://www.swebench.com/)
