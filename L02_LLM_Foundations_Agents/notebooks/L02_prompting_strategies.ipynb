{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# L02: Prompting Strategies Lab\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Digital-AI-Finance/agentic-artificial-intelligence/blob/main/L02_LLM_Foundations_Agents/notebooks/L02_prompting_strategies.ipynb)\n\n**Week 2 - LLM Foundations for Agents**\n\n## Learning Objectives\n- Implement Chain-of-Thought, Tree-of-Thoughts, and Self-Consistency\n- Compare prompting strategies on reasoning tasks\n- Measure accuracy vs. cost trade-offs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colab setup\nimport sys\nif 'google.colab' in sys.modules:\n    !pip install -q openai python-dotenv\n    from google.colab import userdata\n    import os\n    os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport json\nfrom collections import Counter\n\nload_dotenv()\nclient = OpenAI()\nprint(\"Ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_cot(question: str) -> str:\n",
    "    \"\"\"Zero-shot Chain-of-Thought prompting.\"\"\"\n",
    "    prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test\n",
    "question = \"Roger has 5 tennis balls. He buys 2 cans of 3 balls each. How many tennis balls does he have now?\"\n",
    "print(\"Question:\", question)\n",
    "print(\"\\nChain-of-Thought Response:\")\n",
    "print(zero_shot_cot(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-Consistency Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_answer(response: str) -> str:\n",
    "    \"\"\"Extract numerical answer from response.\"\"\"\n",
    "    numbers = re.findall(r'\\b(\\d+)\\b', response)\n",
    "    return numbers[-1] if numbers else None\n",
    "\n",
    "def self_consistency(question: str, n_samples: int = 5) -> str:\n",
    "    \"\"\"Self-consistency with majority voting.\"\"\"\n",
    "    prompt = f\"{question}\\n\\nLet's think step by step:\"\n",
    "    answers = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.7,  # Higher temperature for diversity\n",
    "            max_tokens=300\n",
    "        )\n",
    "        answer = extract_answer(response.choices[0].message.content)\n",
    "        if answer:\n",
    "            answers.append(answer)\n",
    "    \n",
    "    # Majority vote\n",
    "    if answers:\n",
    "        vote_counts = Counter(answers)\n",
    "        return vote_counts.most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "# Test\n",
    "print(\"Self-Consistency (5 samples):\")\n",
    "result = self_consistency(question)\n",
    "print(f\"Final Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tree-of-Thoughts (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_thoughts(question: str, n_thoughts: int = 3) -> list:\n",
    "    \"\"\"Generate multiple initial thoughts.\"\"\"\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "\n",
    "Generate {n_thoughts} different initial approaches to solve this problem.\n",
    "Format as:\n",
    "Approach 1: ...\n",
    "Approach 2: ...\n",
    "Approach 3: ...\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def evaluate_thought(question: str, thought: str) -> float:\n",
    "    \"\"\"Evaluate a thought's promise (0-1).\"\"\"\n",
    "    prompt = f\"\"\"Question: {question}\n",
    "Proposed approach: {thought}\n",
    "\n",
    "Rate this approach's likelihood of success (0.0 to 1.0).\n",
    "Respond with only the number.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    try:\n",
    "        return float(response.choices[0].message.content.strip())\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "# Test\n",
    "print(\"Tree-of-Thoughts Exploration:\")\n",
    "thoughts = generate_thoughts(question)\n",
    "print(thoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparison Experiment\n",
    "\n",
    "Compare strategies on GSM8K-style problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_problems = [\n",
    "    {\"q\": \"A baker has 24 muffins. He sells 1/3 of them in the morning. How many are left?\", \"a\": \"16\"},\n",
    "    {\"q\": \"Lisa has $50. She buys 3 books at $12 each. How much money does she have left?\", \"a\": \"14\"},\n",
    "    {\"q\": \"A train travels 60 mph for 2.5 hours. How far does it go?\", \"a\": \"150\"},\n",
    "]\n",
    "\n",
    "def evaluate_strategy(strategy_fn, problems, strategy_name):\n",
    "    \"\"\"Evaluate a prompting strategy on test problems.\"\"\"\n",
    "    correct = 0\n",
    "    for p in problems:\n",
    "        result = strategy_fn(p[\"q\"])\n",
    "        answer = extract_answer(result) if isinstance(result, str) else result\n",
    "        if answer == p[\"a\"]:\n",
    "            correct += 1\n",
    "    accuracy = correct / len(problems)\n",
    "    print(f\"{strategy_name}: {correct}/{len(problems)} = {accuracy:.1%}\")\n",
    "    return accuracy\n",
    "\n",
    "# Run comparison\n",
    "print(\"Strategy Comparison:\")\n",
    "print(\"=\"*40)\n",
    "evaluate_strategy(zero_shot_cot, test_problems, \"Zero-shot CoT\")\n",
    "evaluate_strategy(self_consistency, test_problems, \"Self-Consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Takeaways\n",
    "\n",
    "- **Zero-shot CoT**: Simple, low cost, good for many tasks\n",
    "- **Self-Consistency**: Higher accuracy, but ~5x cost\n",
    "- **Tree-of-Thoughts**: Best for complex planning, highest cost\n",
    "\n",
    "For agents: Use CoT by default, Self-Consistency for critical decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}